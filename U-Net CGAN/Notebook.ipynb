{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdLQYTne6wTW",
    "outputId": "4e06aba4-167b-49a0-f1f9-35204423bda6"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Custom layers"
   ],
   "metadata": {
    "id": "rVKBBs5S66ZX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from keras.layers import Layer, InputSpec\n",
    "\n",
    "try:\n",
    "    from keras import initializations\n",
    "except ImportError:\n",
    "    from keras import initializers as initializations\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for DenseNet used for BatchNormalization.\n",
    "\n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "\n",
    "        out = in * gamma + beta,\n",
    "\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, weights=None, axis=-1, momentum=0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        # Tensorflow >= 1.0.0 compatibility\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n",
    "        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n",
    "        # self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        # self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self._trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-GbeGkri5mOl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#DenseNet"
   ],
   "metadata": {
    "id": "FcHRzhrc7FnR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import ZeroPadding2D, Concatenate\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "def DenseNet(img_input_layer, img_n_label_input_layer, label_input_layer, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4,\n",
    "             classes=1000, weights_path=None):\n",
    "    \"\"\"Instantiate the DenseNet 121 architecture,\n",
    "        # Arguments\n",
    "            nb_dense_block: number of dense blocks to add to end\n",
    "            growth_rate: number of filters to add per dense block\n",
    "            nb_filter: initial number of filters\n",
    "            reduction: reduction factor of transition blocks.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            classes: optional number of classes to classify images\n",
    "            weights_path: path to pre-trained weights\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "    \"\"\"\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global concat_axis\n",
    "    if K.image_data_format() == 'channels_last': # Tensorflow\n",
    "        concat_axis = 3\n",
    "        # img_input = Input(shape=(224, 224, 3), name='data')\n",
    "        img_input = img_n_label_input_layer\n",
    "    else: # Theano\n",
    "        concat_axis = 1\n",
    "        # img_input = Input(shape=(3, 224, 224), name='data')\n",
    "        img_input = img_n_label_input_layer\n",
    "\n",
    "    # From architecture for ImageNet (Table 1 in the paper)\n",
    "    nb_filter = 64\n",
    "    nb_layers = [6, 12, 24, 16]  # For DenseNet-121\n",
    "\n",
    "    # Initial convolution\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Convolution2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv1_scale')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        stage = block_idx + 2\n",
    "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                                   weight_decay=weight_decay)\n",
    "\n",
    "        # Add transition_block\n",
    "        x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate,\n",
    "                             weight_decay=weight_decay)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "    final_stage = stage + 1\n",
    "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv' + str(final_stage) + '_blk_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name='conv' + str(final_stage) + '_blk_scale')(x)\n",
    "    x = Activation('relu', name='relu' + str(final_stage) + '_blk')(x)\n",
    "    x = GlobalAveragePooling2D(name='pool' + str(final_stage))(x)\n",
    "\n",
    "    x = Dense(classes, name='fc6')(x)\n",
    "    x = Activation('sigmoid', name='prob')(x)\n",
    "\n",
    "    model = Model(inputs=[img_input_layer, label_input_layer], outputs=x, name='densenet')\n",
    "\n",
    "    if weights_path is not None:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4):\n",
    "    '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            branch: layer index within each dense block\n",
    "            nb_filter: number of filters\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_' + str(branch)\n",
    "    relu_name_base = 'relu' + str(stage) + '_' + str(branch)\n",
    "\n",
    "    # 1x1 Convolution (Bottleneck layer)\n",
    "    inter_channel = nb_filter * 4\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base + '_x1_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base + '_x1_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x1')(x)\n",
    "    x = Convolution2D(inter_channel, (1, 1), name=conv_name_base + '_x1', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # 3x3 Convolution\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base + '_x2_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base + '_x2_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base + '_x2')(x)\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '_x2_zeropadding')(x)\n",
    "    x = Convolution2D(nb_filter, (3, 3), name=conv_name_base + '_x2', use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "    ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_filter: number of filters\n",
    "            compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'conv' + str(stage) + '_blk'\n",
    "    relu_name_base = 'relu' + str(stage) + '_blk'\n",
    "    pool_name_base = 'pool' + str(stage)\n",
    "\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base + '_bn')(x)\n",
    "    x = Scale(axis=concat_axis, name=conv_name_base + '_scale')(x)\n",
    "    x = Activation('relu', name=relu_name_base)(x)\n",
    "    x = Convolution2D(int(nb_filter * compression), (1, 1), name=conv_name_base, use_bias=False)(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4,\n",
    "                grow_nb_filters=True):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "        # Arguments\n",
    "            x: input tensor\n",
    "            stage: index for dense block\n",
    "            nb_layers: the number of layers of conv_block to append to the model.\n",
    "            nb_filter: number of filters\n",
    "            growth_rate: growth rate\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "    '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "    concat_feat = x\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        branch = i + 1\n",
    "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay)\n",
    "        concat_feat = Concatenate()([concat_feat, x])\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    return concat_feat, nb_filter"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "StdVnd1h5mOs"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "aNCTXhAT5mOu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from numpy import expand_dims\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "\n",
    "image_size = 224\n",
    "noise_size = 400\n",
    "dataset_labels = 7\n",
    "clip_value = 0.01\n",
    "\n",
    "# Define datagen. Here we can define any transformations we want to apply to images\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "# define training directory that contains subfolders\n",
    "train_dir = os.path.join(\"gdrive\", \"My Drive\", \"Datasets\", \"HAM10000\", \"reorganized\")\n",
    "\n",
    "\n",
    "def load_real_data_old():\n",
    "    # load dataset\n",
    "    (trainX, trainy), (_, _) = load_data()\n",
    "    # expand to 3d, e.g. add channels\n",
    "    X = expand_dims(trainX, axis=-1)\n",
    "    # convert from ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return [X, trainy]\n",
    "\n",
    "\n",
    "def load_real_data():\n",
    "    # emulation dataset loading\n",
    "    train_data_keras = datagen.flow_from_directory(directory=train_dir,\n",
    "                                                   class_mode='categorical',\n",
    "                                                   batch_size=1,  # 16 images at a time\n",
    "                                                   target_size=(image_size, image_size),\n",
    "                                                   color_mode='grayscale')  # Resize images\n",
    "    # split into images and labels\n",
    "    images, labels = next(train_data_keras)\n",
    "    size = train_data_keras.samples\n",
    "    images = numpy.zeros([size, images[0][0].size, images[0][0].size, 1])\n",
    "    return [images, labels]\n",
    "\n",
    "\n",
    "def get_images_and_labels(n_samples):\n",
    "    train_data_keras = datagen.flow_from_directory(directory=train_dir,\n",
    "                                                   class_mode='sparse',\n",
    "                                                   batch_size=n_samples,  # 16 images at a time\n",
    "                                                   target_size=(image_size, image_size),\n",
    "                                                   color_mode='grayscale')  # Resize images\n",
    "    # split into images and labels\n",
    "    images, labels = next(train_data_keras)\n",
    "    # labels = numpy.argmax(labels, axis=-1)\n",
    "    # convert from ints to floats\n",
    "    images = images.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    images = (images - 127.5) / 127.5\n",
    "    # generate class labels\n",
    "    return images, labels.astype(int)\n",
    "\n",
    "\n",
    "def generate_real_samples(n_samples):\n",
    "    images, labels = get_images_and_labels(n_samples)\n",
    "    # generate class labels\n",
    "    y = -ones((n_samples, 1))\n",
    "    return [images, labels], y\n",
    "\n",
    "\n",
    "def get_noise(latent_dim, n_samples):\n",
    "    return randn(latent_dim * n_samples).reshape(n_samples, latent_dim)\n",
    "\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    img_input, labels_input = get_images_and_labels(n_samples)\n",
    "    z_input = get_noise(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict([img_input, labels_input, z_input])\n",
    "    # create class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [images, labels_input], y\n",
    "\n",
    "\n",
    "def get_noise_data(batch_size):\n",
    "    return np.random.normal(0, 0.001, size=[batch_size, noise_size]).astype(np.float32)\n",
    "\n",
    "\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot to file\n",
    "    filename = 'generated_plot_e%03d.png' % (epoch + 1)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "from enum import IntEnum\n",
    "\n",
    "\n",
    "class FashionLabel(IntEnum):\n",
    "    Tshirt = 0\n",
    "    Trouser = 1\n",
    "    Pullover = 2\n",
    "    Dress = 3\n",
    "    Coat = 4\n",
    "    Sandal = 5\n",
    "    Shirt = 6\n",
    "    Sneaker = 7\n",
    "    Bag = 8\n",
    "    Ankle_boot = 9\n",
    "\n",
    "class CancerLabel(IntEnum):\n",
    "    akiec = 0\n",
    "    bcc = 1\n",
    "    bkl = 2\n",
    "    df = 3\n",
    "    mel = 4\n",
    "    nv = 5\n",
    "    vasc = 6"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b6JcpiTw5mOx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Nets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "zu9LYlNZ5mOy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend\n",
    "from keras.constraints import Constraint\n",
    "from keras.layers import Input, Dense, Concatenate, ReLU, Conv2D, Conv2DTranspose, Reshape, BatchNormalization, \\\n",
    "    Activation, Embedding, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow import Tensor, keras\n",
    "\n",
    "def relu_bn(inputs: Tensor) -> Tensor:\n",
    "    relu = ReLU()(inputs)\n",
    "    bn = BatchNormalization()(relu)\n",
    "    return bn\n",
    "\n",
    "\n",
    "drop_out_rate = 0.5\n",
    "w_initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.02)\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    "\n",
    "\n",
    "def up_scaling_layer(x, n_filters):\n",
    "    kernel = 1\n",
    "    stride = 2\n",
    "    x = Conv2DTranspose(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def down_scaling_layer(x, n_filters):\n",
    "    kernel = 1\n",
    "    stride = 2\n",
    "    x = Conv2D(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resNet_block(x, n_filters, scaling):\n",
    "    kernel = 3\n",
    "    stride = 1\n",
    "    x = Conv2D(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = Conv2D(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = Conv2D(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = Conv2D(n_filters, kernel_size=kernel, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if scaling == 'up':\n",
    "        x = up_scaling_layer(x, n_filters)\n",
    "    else:\n",
    "        x = down_scaling_layer(x, n_filters)\n",
    "    print(\"x shape: \", x.shape)\n",
    "    return x\n",
    "\n",
    "\n",
    "def representation_layer(layer, noise_input):\n",
    "    filter = 3\n",
    "    stride = 2\n",
    "    kernel = 4\n",
    "\n",
    "    dense = Dense(7 * 7 * filter, kernel_initializer=w_initializer)(noise_input)\n",
    "\n",
    "    reshape = Reshape((7, 7, filter))(dense)\n",
    "\n",
    "    noise_conv = Conv2DTranspose(kernel_size=kernel,\n",
    "                                 strides=stride,\n",
    "                                 filters=2 * filter,\n",
    "                                 padding=\"same\",\n",
    "                                 kernel_initializer=w_initializer)(reshape)\n",
    "    concat_layer = Concatenate()([layer, noise_conv])\n",
    "    print(\"concat_layer shape: \", concat_layer.shape)\n",
    "    return concat_layer\n",
    "\n",
    "\n",
    "def img_n_label_layer(input_shape, n_classes):\n",
    "    # label input\n",
    "    label_input_layer = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(label_input_layer)\n",
    "    # scale up to image dimensions with linear activation\n",
    "    n_nodes = input_shape[0] * input_shape[1]\n",
    "    li = Dense(n_nodes)(li)\n",
    "    # reshape to additional channel\n",
    "    li = Reshape((input_shape[0], input_shape[1], 1))(li)\n",
    "    # image input\n",
    "    img_input_layer = Input(shape=input_shape)\n",
    "    # concat label as a channel\n",
    "    img_n_label_input_layer = Concatenate()([img_input_layer, li])\n",
    "    return img_input_layer, img_n_label_input_layer, label_input_layer\n",
    "\n",
    "\n",
    "def define_generator(latent_dim, input_shape, n_classes):\n",
    "    n_filters = 3\n",
    "\n",
    "    img_input_layer, img_n_label_input_layer, label_input_layer = img_n_label_layer(input_shape, n_classes)\n",
    "\n",
    "    x1 = resNet_block(img_n_label_input_layer, n_filters, 'down')\n",
    "    x2 = resNet_block(x1, 2 * n_filters, 'down')\n",
    "    x3 = resNet_block(x2, 4 * n_filters, 'down')\n",
    "    x4 = resNet_block(x3, 8 * n_filters, 'down')\n",
    "\n",
    "    noise_input_layer = Input(shape=latent_dim)\n",
    "    decoded_img_and_noise = representation_layer(x4, noise_input_layer)\n",
    "\n",
    "    x5 = resNet_block(decoded_img_and_noise, 8 * n_filters, 'up')\n",
    "    skip_connection_3_5 = Concatenate(axis=-1)([x3, x5])\n",
    "    x6 = resNet_block(skip_connection_3_5, 4 * n_filters, 'up')\n",
    "    skip_connection_2_6 = Concatenate(axis=-1)([x2, x6])\n",
    "    x7 = resNet_block(skip_connection_2_6, 2 * n_filters, 'up')\n",
    "    skip_connection_1_7 = Concatenate(axis=-1)([x1, x7])\n",
    "    x8 = resNet_block(skip_connection_1_7, 1, 'up')\n",
    "\n",
    "    gen_output = Activation('tanh')(x8)\n",
    "\n",
    "    model = Model(inputs=[img_input_layer, label_input_layer, noise_input_layer], outputs=gen_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_discriminator(input_shape, n_classes):\n",
    "    img_input_layer, img_n_label_input_layer, label_input_layer = img_n_label_layer(input_shape, n_classes)\n",
    "\n",
    "    model = DenseNet(img_input_layer, img_n_label_input_layer, label_input_layer, reduction=0.5, classes=1)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99)\n",
    "    model.compile(optimizer=opt, loss=wasserstein_loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # d_model.trainable = False\n",
    "    # get img, label and noise inputs from generator model\n",
    "    gen_img, gen_label, gen_noise = g_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = g_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    # define gan model as taking img, noise and label and outputting a classification\n",
    "    model = Model([gen_img, gen_label, gen_noise], gan_output)\n",
    "    # compile model\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    return model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Dm0JKqoa5mOz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ivVF3G7Q5mO0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 10015 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# example of training an conditional gan on the fashion mnist dataset\n",
    "from matplotlib import pyplot\n",
    "from numpy import ones, mean\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.plot(d1_hist, label='crit_real')\n",
    "    pyplot.plot(d2_hist, label='crit_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('plot_line_plot_loss.png')\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=100, n_batch=128):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    n_critic = 5\n",
    "    # lists for keeping track of loss\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    X_real, labels_real, y_real = [], [], []\n",
    "    for i in range(n_steps):\n",
    "        # update the critic more than the generator\n",
    "        c1_tmp, c2_tmp = list(), list()\n",
    "        for _ in range(n_critic):\n",
    "            # get randomly selected 'real' samples\n",
    "            [X_real, labels_real], y_real = generate_real_samples(half_batch)\n",
    "            # update critic model weights\n",
    "            c_loss1 = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            c1_tmp.append(c_loss1)\n",
    "            # generate 'fake' examples\n",
    "            [X_fake, labels_fake], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update critic model weights\n",
    "            c_loss2 = d_model.train_on_batch([X_fake, labels_fake], y_fake)\n",
    "            c2_tmp.append(c_loss2)\n",
    "            # store critic loss\n",
    "            c1_hist.append(mean(c1_tmp))\n",
    "            c2_hist.append(mean(c2_tmp))\n",
    "\n",
    "            for l in d_model.layers:\n",
    "                weights = l.get_weights()\n",
    "                print(\"Unclipped weights \", weights)\n",
    "                weights = [tf.clip_by_value(w, -clip_value, clip_value) for w in weights]\n",
    "                l.set_weights(weights)\n",
    "                print(\"Clipped weights \", l.get_weights)\n",
    "                \n",
    "        # prepare points in latent space as input for the generator\n",
    "        img_input, labels_input, y_gan = X_real, labels_real, y_real\n",
    "        # img_input, labels_input, y_gan = Utils.generate_real_samples(n_batch)\n",
    "        z_input = get_noise(latent_dim, half_batch)\n",
    "        # update the generator via the critic's error\n",
    "        g_loss = gan_model.train_on_batch([img_input, labels_input, z_input], y_gan)\n",
    "        g_hist.append(g_loss)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d/%d, c1=%.3f, c2=%.3f g=%.3f' % (i + 1, n_steps, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "        # evaluate the model performance every 'epoch'\n",
    "        if (i + 1) % bat_per_epo == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset)\n",
    "    # line plots of loss\n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "\n",
    "\n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, n_samples=15):\n",
    "    # prepare real samples\n",
    "    # X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # # evaluate discriminator on real examples\n",
    "    # _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # # prepare fake examples\n",
    "    # x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # # evaluate discriminator on fake examples\n",
    "    # _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # # summarize discriminator performance\n",
    "    # print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real * 100, acc_fake * 100))\n",
    "    # save plot\n",
    "    # save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'dis_model_%03d.h5' % (epoch + 1)\n",
    "    d_model.save(filename)\n",
    "    filename = 'gen_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    "\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# load image data\n",
    "dataset = load_real_data()\n",
    "image_shape = dataset[0][0].shape\n",
    "# create the discriminator\n",
    "d_model = define_discriminator(image_shape, dataset_labels)\n",
    "# create the generator\n",
    "g_model = define_generator(latent_dim, image_shape, dataset_labels)\n",
    "# create the gan\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, latent_dim)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBPasL-q5mO1",
    "outputId": "817dbcd4-6730-428e-8582-09bae11e2283"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}